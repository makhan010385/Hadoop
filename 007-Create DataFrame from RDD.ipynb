{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a84a357-076e-43e9-8299-effbe4309370",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dcab7c9-cee0-49f0-a105-86ff1951caf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: [2, 3, 4, 5, 6, 7, 8, 9]"
     ]
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2e63e6-7dfb-4f9e-a9e7-c1fdfe408897",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#how many element in the RDD we can see with the help of count funtion or action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd1c892-ddc0-49b0-8f04-a9f0480936dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 8"
     ]
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75ee52b-8c5d-45ff-ac0a-8fac07b6d5e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd1 =sc.parallelize([(1,\"India\"),(2,\"USA\"),(3, \"UK\"),(4,\"Germany\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2d2069-a33e-4f35-b1e8-3b5c4c32e291",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [(1, 'India'), (2, 'USA'), (3, 'UK'), (4, 'Germany')]"
     ]
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caea38d4-fb4e-427b-9c22-5f0231ca2c01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Now we convert Rdd to Dataframe\n",
    "df =rdd1.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29220191-2386-4593-a2f7-0bf01c4c4059",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: DataFrame[_1: bigint, _2: string]"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab11e36-a96d-4488-a430-00628faf8554",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: [Row(_1=1, _2='India'),\n Row(_1=2, _2='USA'),\n Row(_1=3, _2='UK'),\n Row(_1=4, _2='Germany')]"
     ]
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "982918d3-7544-4aee-bc5a-49080a41feab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| _1|     _2|\n+---+-------+\n|  1|  India|\n|  2|    USA|\n|  3|     UK|\n|  4|Germany|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#if you want to data frame into tabular from then we will use show method of the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1314e7f4-0779-436c-8a81-33f281023dcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_1</th><th>_2</th></tr></thead><tbody><tr><td>1</td><td>India</td></tr><tr><td>2</td><td>USA</td></tr><tr><td>3</td><td>UK</td></tr><tr><td>4</td><td>Germany</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "India"
        ],
        [
         2,
         "USA"
        ],
        [
         3,
         "UK"
        ],
        [
         4,
         "Germany"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_1",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "_2",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Display is used to show data into tabular form and show() is showing the data into embeded form\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673af4ed-157b-4725-96c9-388d3aa5a87b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: 4"
     ]
    }
   ],
   "source": [
    "#we can count number of rows with the help of count function that also performed by the RDD. Count function showing number of record into dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163eb464-e3fa-4025-83fa-c86b265319c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method show in module pyspark.sql.dataframe:\n\nshow(n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None method of pyspark.sql.dataframe.DataFrame instance\n    Prints the first ``n`` rows to the console.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    n : int, optional\n        Number of rows to show.\n    truncate : bool or int, optional\n        If set to ``True``, truncate strings longer than 20 chars by default.\n        If set to a number greater than one, truncates long strings to length ``truncate``\n        and align cells right.\n    vertical : bool, optional\n        If set to ``True``, print output rows vertically (one line\n        per column value).\n    \n    Returns\n    -------\n    None\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([\n    ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n    \n    Show only top 2 rows.\n    \n    >>> df.show(2)\n    +---+-----+\n    |age| name|\n    +---+-----+\n    | 14|  Tom|\n    | 23|Alice|\n    +---+-----+\n    only showing top 2 rows\n    \n    Show :class:`DataFrame` where the maximum number of characters is 3.\n    \n    >>> df.show(truncate=3)\n    +---+----+\n    |age|name|\n    +---+----+\n    | 14| Tom|\n    | 23| Ali|\n    | 16| Bob|\n    +---+----+\n    \n    Show :class:`DataFrame` vertically.\n    \n    >>> df.show(vertical=True)\n    -RECORD 0-----\n    age  | 14\n    name | Tom\n    -RECORD 1-----\n    age  | 23\n    name | Alice\n    -RECORD 2-----\n    age  | 16\n    name | Bob\n\n"
     ]
    }
   ],
   "source": [
    "help(df.show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "454b4d52-b3c2-412c-a78a-8e04950516fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| _1|   _2|\n+---+-----+\n|  1|India|\n|  2|  USA|\n+---+-----+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# we can see the record using show function\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd83c000-491a-4d27-8024-1e9865ed5ef4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema imposing with the help of giving column name to dataframe\n",
    "myschema =[\"id\",\"Country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd77ff27-6cf8-4b63-93c0-77c9b115e143",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#imposing the schema into dataframe\n",
    "df2 =rdd1.toDF(myschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe7b3b1-8e29-4da9-9a4d-a1157f534059",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|Country|\n+---+-------+\n|  1|  India|\n|  2|    USA|\n|  3|     UK|\n|  4|Germany|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a558e9b6-bf0b-4adb-a010-5ecd2974dd19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _1: long (nullable = true)\n |-- _2: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#How to show shema of the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3db6c5e-01df-4d64-89f5-3c5ec8d0ae78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#You can create daframe in the otherway too\n",
    "df4 = rdd1.toDF(\"id integer , country string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff0b222a-b30d-43ea-a37e-5fb845aa9918",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|country|\n+---+-------+\n|  1|  India|\n|  2|    USA|\n|  3|     UK|\n|  4|Germany|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8f776c-ee4c-47bf-98a5-8cd3284915bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#we can see the schema for above dataframe\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac83cb1-7bec-40ab-be85-74719fdce8ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we can make the schema in different ways \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, DateType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ab7c95-7c50-4e76-8555-ac30267e0e93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if we want to all the data with single import then we can write \n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559140b6-1451-4373-8663-0f2d0ee96c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3457178532009956>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#if you having two column schema then we can write \u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m mydfSchema \u001B[38;5;241m=\u001B[39m StructType([\n",
       "\u001B[0;32m----> 3\u001B[0m \u001B[43mStructField\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mLongType\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m,\n",
       "\u001B[1;32m      4\u001B[0m StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m,LongType,\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m      5\u001B[0m ])\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:622\u001B[0m, in \u001B[0;36mStructField.__init__\u001B[0;34m(self, name, dataType, nullable, metadata)\u001B[0m\n",
       "\u001B[1;32m    615\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n",
       "\u001B[1;32m    616\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    617\u001B[0m     name: \u001B[38;5;28mstr\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    620\u001B[0m     metadata: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, Any]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    621\u001B[0m ):\n",
       "\u001B[0;32m--> 622\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataType, DataType), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataType \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m should be an instance of \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\n",
       "\u001B[1;32m    623\u001B[0m         dataType,\n",
       "\u001B[1;32m    624\u001B[0m         DataType,\n",
       "\u001B[1;32m    625\u001B[0m     )\n",
       "\u001B[1;32m    626\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(name, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield name \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (name)\n",
       "\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m name\n",
       "\n",
       "\u001B[0;31mAssertionError\u001B[0m: dataType <class 'pyspark.sql.types.LongType'> should be an instance of <class 'pyspark.sql.types.DataType'>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3457178532009956>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#if you having two column schema then we can write \u001B[39;00m\n\u001B[1;32m      2\u001B[0m mydfSchema \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[0;32m----> 3\u001B[0m \u001B[43mStructField\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mLongType\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m,\n\u001B[1;32m      4\u001B[0m StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m,LongType,\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m ])\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:622\u001B[0m, in \u001B[0;36mStructField.__init__\u001B[0;34m(self, name, dataType, nullable, metadata)\u001B[0m\n\u001B[1;32m    615\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    616\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    617\u001B[0m     name: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    620\u001B[0m     metadata: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, Any]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    621\u001B[0m ):\n\u001B[0;32m--> 622\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataType, DataType), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataType \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m should be an instance of \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\n\u001B[1;32m    623\u001B[0m         dataType,\n\u001B[1;32m    624\u001B[0m         DataType,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(name, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield name \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (name)\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m name\n\n\u001B[0;31mAssertionError\u001B[0m: dataType <class 'pyspark.sql.types.LongType'> should be an instance of <class 'pyspark.sql.types.DataType'>",
       "errorSummary": "<span class='ansi-red-fg'>AssertionError</span>: dataType <class 'pyspark.sql.types.LongType'> should be an instance of <class 'pyspark.sql.types.DataType'>",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#if you having two column schema then we can write \n",
    "mydfSchema = StructType([\n",
    "StructField(\"id\",LongType, True),\n",
    "StructField(\"id\",LongType,True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b82c26-11dc-4917-ac73-c32035a4c172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3457178532009957>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#if you having two column schema then we can write \u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m mydfSchema \u001B[38;5;241m=\u001B[39m StructType([\n",
       "\u001B[0;32m----> 3\u001B[0m \u001B[43mStructField\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mLongType\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m,\n",
       "\u001B[1;32m      4\u001B[0m StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcountry\u001B[39m\u001B[38;5;124m\"\u001B[39m,StringType,\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m      5\u001B[0m ])\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:622\u001B[0m, in \u001B[0;36mStructField.__init__\u001B[0;34m(self, name, dataType, nullable, metadata)\u001B[0m\n",
       "\u001B[1;32m    615\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n",
       "\u001B[1;32m    616\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    617\u001B[0m     name: \u001B[38;5;28mstr\u001B[39m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    620\u001B[0m     metadata: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, Any]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[1;32m    621\u001B[0m ):\n",
       "\u001B[0;32m--> 622\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataType, DataType), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataType \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m should be an instance of \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\n",
       "\u001B[1;32m    623\u001B[0m         dataType,\n",
       "\u001B[1;32m    624\u001B[0m         DataType,\n",
       "\u001B[1;32m    625\u001B[0m     )\n",
       "\u001B[1;32m    626\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(name, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield name \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (name)\n",
       "\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m name\n",
       "\n",
       "\u001B[0;31mAssertionError\u001B[0m: dataType <class 'pyspark.sql.types.LongType'> should be an instance of <class 'pyspark.sql.types.DataType'>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3457178532009957>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#if you having two column schema then we can write \u001B[39;00m\n\u001B[1;32m      2\u001B[0m mydfSchema \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[0;32m----> 3\u001B[0m \u001B[43mStructField\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mid\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mLongType\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m,\n\u001B[1;32m      4\u001B[0m StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcountry\u001B[39m\u001B[38;5;124m\"\u001B[39m,StringType,\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m ])\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:622\u001B[0m, in \u001B[0;36mStructField.__init__\u001B[0;34m(self, name, dataType, nullable, metadata)\u001B[0m\n\u001B[1;32m    615\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    616\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    617\u001B[0m     name: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    620\u001B[0m     metadata: Optional[Dict[\u001B[38;5;28mstr\u001B[39m, Any]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    621\u001B[0m ):\n\u001B[0;32m--> 622\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataType, DataType), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataType \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m should be an instance of \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\n\u001B[1;32m    623\u001B[0m         dataType,\n\u001B[1;32m    624\u001B[0m         DataType,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(name, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfield name \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (name)\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m name\n\n\u001B[0;31mAssertionError\u001B[0m: dataType <class 'pyspark.sql.types.LongType'> should be an instance of <class 'pyspark.sql.types.DataType'>",
       "errorSummary": "<span class='ansi-red-fg'>AssertionError</span>: dataType <class 'pyspark.sql.types.LongType'> should be an instance of <class 'pyspark.sql.types.DataType'>",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#if you having two column schema then we can write \n",
    "mydfSchema = StructType([\n",
    "StructField(\"id\",LongType, True),\n",
    "StructField(\"country\",StringType,True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f7f421b-6b3f-4e79-b4c3-4b769084b756",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#if you having two column schema then we can write \n",
    "mydfSchema = StructType([\n",
    "StructField(\"id\",LongType(), True),\n",
    "StructField(\"country\",StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49426ad-1d60-46cb-a917-cd71e1461a9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df5 = rdd1.toDF(mydfSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a341f9f-23e9-4154-bc25-401b40b9304d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|country|\n+---+-------+\n|  1|  India|\n|  2|    USA|\n|  3|     UK|\n|  4|Germany|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48ce527-01ff-4bf4-b076-bf04f5ffccc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56861c61-1f69-4868-af1f-91cc2a039193",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n\ncreateDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n    or a :class:`numpy.ndarray`.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    data : :class:`RDD` or iterable\n        an RDD of any kind of SQL data representation (:class:`Row`,\n        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n        column names, default is None.  The data type string format equals to\n        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n        omit the ``struct<>``.\n    \n        When ``schema`` is a list of column names, the type of each column\n        will be inferred from ``data``.\n    \n        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n        from ``data``, which should be an RDD of either :class:`Row`,\n        :class:`namedtuple`, or :class:`dict`.\n    \n        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n        match the real data, or an exception will be thrown at runtime. If the given schema is\n        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n        later.\n    samplingRatio : float, optional\n        the sample ratio of rows used for inferring. The first few rows will be used\n        if ``samplingRatio`` is ``None``.\n    verifySchema : bool, optional\n        verify data types of every row against schema. Enabled by default.\n    \n        .. versionadded:: 2.1.0\n    \n    Returns\n    -------\n    :class:`DataFrame`\n    \n    Notes\n    -----\n    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n    \n    Examples\n    --------\n    Create a DataFrame from a list of tuples.\n    \n    >>> spark.createDataFrame([('Alice', 1)]).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from a list of dictionaries\n    \n    >>> d = [{'name': 'Alice', 'age': 1}]\n    >>> spark.createDataFrame(d).collect()\n    [Row(age=1, name='Alice')]\n    \n    Create a DataFrame from an RDD.\n    \n    >>> rdd = spark.sparkContext.parallelize([('Alice', 1)])\n    >>> spark.createDataFrame(rdd).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n    >>> df.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from Row instances.\n    \n    >>> from pyspark.sql import Row\n    >>> Person = Row('name', 'age')\n    >>> person = rdd.map(lambda r: Person(*r))\n    >>> df2 = spark.createDataFrame(person)\n    >>> df2.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame with the explicit schema specified.\n    \n    >>> from pyspark.sql.types import *\n    >>> schema = StructType([\n    ...    StructField(\"name\", StringType(), True),\n    ...    StructField(\"age\", IntegerType(), True)])\n    >>> df3 = spark.createDataFrame(rdd, schema)\n    >>> df3.collect()\n    [Row(name='Alice', age=1)]\n    \n    Create a DataFrame from a pandas DataFrame.\n    \n    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n    [Row(name='Alice', age=1)]\n    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n    [Row(0=1, 1=2)]\n    \n    Create  a DataFrame from an RDD with the schema in DDL formatted string.\n    \n    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n    [Row(a='Alice', b=1)]\n    >>> rdd = rdd.map(lambda row: row[1])\n    >>> spark.createDataFrame(rdd, \"int\").collect()\n    [Row(value=1)]\n    \n    When the type is unmatched, it throws an exception.\n    \n    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    Py4JJavaError: ...\n\n"
     ]
    }
   ],
   "source": [
    "#how to directly create dataframe \n",
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ccbec3-aeee-4914-81e6-048ab347f8ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df6 =spark.createDataFrame(rdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b7605a-53be-472b-ba6d-7958843d41c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| _1|     _2|\n+---+-------+\n|  1|  India|\n|  2|    USA|\n|  3|     UK|\n|  4|Germany|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d731512-8db9-4dc5-af87-fba1e6f9face",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _1: long (nullable = true)\n |-- _2: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d60ca0a-8e20-4d29-8a48-606c309b20fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#if we want to give the column name then we shud write that means we pass schema \n",
    "df7 =spark.createDataFrame(rdd1,myschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e12f6a43-f8d9-4fd1-a385-7de939f3c32a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|Country|\n+---+-------+\n|  1|  India|\n|  2|    USA|\n|  3|     UK|\n|  4|Germany|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b52892-43c2-4fb4-8a58-8567e870b645",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- Country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df7.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c47d464-529e-45a0-89cf-4d8683f3c8c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8 =spark.createDataFrame(rdd1,\"id long, Country string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed329390-32e8-4306-9f48-60019be82e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|Country|\n+---+-------+\n|  1|  India|\n|  2|    USA|\n|  3|     UK|\n|  4|Germany|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e5c45d-fd7f-40e8-a89d-60297511d1ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "newdf =spark.createDataFrame([Row(1,\"India\"),Row(2,\"UK\"),Row(3,\"US\")],myschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df362c82-1270-4d37-93fd-b0f15eb8f6ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|Country|\n+---+-------+\n|  1|  India|\n|  2|     UK|\n|  3|     US|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d4a4a1a-9a60-4e1a-a2d3-7719e6e05e68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "007-Create DataFrame from RDD",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
